{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b2f6c1",
   "metadata": {},
   "source": [
    "# Cryptocurrency Data Transformation Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4010dc",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "|    Student Name                 |    Student Number  |\n",
    "|---------------------------------|--------------------|\n",
    "| Raj Sandhu                      | 101111960          |\n",
    "| Akaash Kapoor                   | 101112895          |\n",
    "| Ali Alvi                        | 101114940          |\n",
    "| Hassan Jallad                   | 101109334          |\n",
    "| Areeb Ul Haq                    | 101115337          |\n",
    "| Ahmad Abuoudeh                  | 101072636          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf77393",
   "metadata": {},
   "source": [
    "# Libraries To Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5df6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ed665",
   "metadata": {},
   "source": [
    "# Configure Destination Paths for Transformed Data to be Stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3ae2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "data_folder = \"data\"\n",
    "raw_folder = \"raw\"\n",
    "interim_folder = \"interim\"\n",
    "processed_folder = \"processed\"\n",
    "bin_folder = \"bin\"\n",
    "\n",
    "raw_data_file_path = os.path.join(parent_folder, data_folder, raw_folder)\n",
    "interim_data_file_path = os.path.join(parent_folder, data_folder, interim_folder)\n",
    "processed_data_file_path = os.path.join(parent_folder, data_folder, processed_folder)\n",
    "bin_file_path = os.path.join(parent_folder, data_folder, bin_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc0ab8",
   "metadata": {},
   "source": [
    "# Data Transformation: Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845dc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Part 1, the script will extract interim data already ingested, and extract the coin's name and average\n",
    "#volatility. This will be stored in two lists, one for names, one for description, and one for volatility. \n",
    "\n",
    "#Lists to store names and volatilities.    \n",
    "coin_names = []\n",
    "coin_volatilities = []\n",
    "\n",
    "#Open the raw data folder. \n",
    "coin_raw_files = os.listdir(raw_data_file_path)\n",
    "\n",
    "#Extract names from all csv files. \n",
    "for file in coin_raw_files:\n",
    "    coin_names.append(file.replace(\".csv\",''))\n",
    "\n",
    "#Calculates the volatility of all the coins by extracting average prices for each coin and calculate standard deviation. \n",
    "for name in coin_names:\n",
    "    with open(os.path.join(raw_data_file_path, name + \".csv\"), \"r\") as file:\n",
    "        #Checks if the csv file is not empty, otherwise, append 0. \n",
    "        try:\n",
    "            df= pd.read_csv(file)\n",
    "            coin_volatilities.append(df[\"price_avg_usd\"].std())\n",
    "        except:\n",
    "            coin_volatilities.append(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003aabd6",
   "metadata": {},
   "source": [
    "# Data Transformation: Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99042643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In Part 2, the script will create a dataframe (using pandas) and join it with the interim data. \n",
    "#This dataframe will also be stored in the processed data file path. This data will be used for building the\n",
    "#recommendation system and running analytics in project phase 3.\n",
    "\n",
    "#Author: Ali Alvi\n",
    "\n",
    "\n",
    "#Create a data frame with two categories to store coin name and volatility\n",
    "coin_data = pd.DataFrame({'Name': coin_names,\n",
    "                          'Volatility': coin_volatilities})\n",
    "\n",
    "with open(os.path.join(interim_data_file_path, \"coin-data.csv\"), \"w\") as f :\n",
    "    coin_data.to_csv(f, index = False) #Write it to interim folder since data has been transformed and undergone intermediate cleaning.\n",
    "\n",
    "#Now we import the two csv files in the interim folder\n",
    "\n",
    "coin_data_csv1 = pd.read_csv(interim_data_file_path + '/coin-data.csv')\n",
    "coin_desc_csv2 = pd.read_csv(interim_data_file_path + '/coin-descriptions.csv')\n",
    "\n",
    "\n",
    "#Now we will perform an inner join/natural join between two csv files so we can generate a\n",
    "#dataset with Name as the common column\n",
    "\n",
    "coin_info_processed = pd.merge(coin_data_csv1,coin_desc_csv2,\n",
    "                               on='Name', how='inner')\n",
    "\n",
    "#Retrieve rows that have volatility as 0 and store them in a seperate csv file as bin resource\n",
    "\n",
    "removed_rows = coin_info_processed.loc[coin_info_processed['Volatility'] == 0]\n",
    "\n",
    "with open(os.path.join(bin_file_path, \"removed_coins.csv\"), \"w\") as f :\n",
    "    removed_rows.to_csv(f, index = False)\n",
    "\n",
    "#Remove the previously retrieved rows\n",
    "\n",
    "#Listwise deletion is used here if the volatility of a coin cannot be computed. This is because the missing data only consists of around 5-6% of the\n",
    "#overall data, meaning it is within the threshold to be safely removed without introducing much bias into the dataset.\n",
    "\n",
    "coin_info_processed.drop(coin_info_processed.index[coin_info_processed['Volatility'] == 0], inplace=True)\n",
    "\n",
    "#Store the processed data to the processes folder\n",
    "with open(os.path.join(processed_data_file_path, \"coin-info.csv\"), \"w\") as f :\n",
    "    coin_info_processed.to_csv(f, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a5e81-7ca7-4dd9-ad5f-6226f9da0a34",
   "metadata": {},
   "source": [
    "## Assessing Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89fec8fe-eff6-472a-ae2c-2ebcb7641e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.410000e+02\n",
       "mean     3.632290e+02\n",
       "std      1.644140e+03\n",
       "min      3.640742e-11\n",
       "25%      1.715058e-01\n",
       "50%      1.375242e+00\n",
       "75%      2.388440e+01\n",
       "max      9.356312e+03\n",
       "Name: Volatility, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin_df = pd.read_csv(os.path.join(processed_data_file_path, \"coin-info.csv\"))\n",
    "coin_df[\"Volatility\"].describe()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b3ce8d1-4163-42c0-aa91-d687513fe15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                   141\n",
       "unique                                                  141\n",
       "top       IOTA (IOTA or MIOTA) is a cryptocurrency token...\n",
       "freq                                                      1\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coin_df[\"Description\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff1ead-7fd3-43f5-92fb-26f9204ee307",
   "metadata": {},
   "source": [
    "In the above two cells, the describe() function provided by pandas is used to assess the overall quality of the data scraped from CoinCodex. For the Volatility data, it provides some summary statistics such as splitting the data into the 25th, 50th, and 705th percentiles, and providing the count of the data, mean and standard deviation. Applying the describe() function on the coin descriptions provides some basic summaries such as the frequency of a description and the number of unique descriptions. From this basic analysis, the quality of data obtained seems to be high and is ready for analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
