{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89ba68b6-baf8-4c0e-9e70-af5adf359e83",
   "metadata": {},
   "source": [
    "# Cryptocurrency Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4a8c9-34cf-4cd1-b219-062675a7bf1c",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "|    Student Name                 |    Student Number  |\n",
    "|---------------------------------|--------------------|\n",
    "| Raj Sandhu                      | 101111960          |\n",
    "| Akaash Kapoor                   | 101112895          |\n",
    "| Ali Alvi                        | 101114940          |\n",
    "| Hassan Jallad                   | 101109334          |\n",
    "| Areeb Ul Haq                    | 101115337          |\n",
    "| Ahmad Abuoudeh                  | 101072636          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1bfdfe-202f-44c6-8079-e5bca4d26f41",
   "metadata": {},
   "source": [
    "## Libraries to Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed7a7d2d-60b5-4979-a041-bda3971d6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd736a-df24-478e-a1fa-b98ab53a4e43",
   "metadata": {},
   "source": [
    "## Initialize Logging Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fb0334-52d1-42a3-aa35-5fc63769097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This data ingestion notebook incorporates logging in order to observw which the data that is being ingested, as well as to note any errors that may\n",
    "#occur. This is following best practices in industry to ensure only the required data is being ingested, and any errors that occur can be easily\n",
    "#resolved.\n",
    "\n",
    "#Configure path for the log file to be stored.\n",
    "\n",
    "parent_folder = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "log_parent_folder = \"references\"\n",
    "log_folder = \"logs\"\n",
    "log_file_path = os.path.join(parent_folder, log_parent_folder, log_folder)\n",
    "\n",
    "if os.path.isdir(log_file_path) == False :\n",
    "    os.mkdir(log_file_path) #Need to create log directory since it was not pre-made.\n",
    "\n",
    "#Functions to print informative messages and errors if they occur in the log file.\n",
    "def log_info(message) :\n",
    "    log.info(message)\n",
    "    \n",
    "def log_warning(message) :\n",
    "    log.warning(message)\n",
    "\n",
    "def log_error(message):\n",
    "    log.error(message)\n",
    "    log.info(\"Notebook has ended.\")\n",
    "    logstream.close()\n",
    "\n",
    "#Set log handlers and remove any preexisting handlers if they exist\n",
    "log = logging.getLogger(\"cryptocurrency_data_ingestion\")\n",
    "log.setLevel(logging.INFO)\n",
    "\n",
    "if log.hasHandlers():\n",
    "    for handler in log.handlers:\n",
    "        log.removeHandler(handler)\n",
    "\n",
    "logstream = open(log_file_path + \"\\data-ingestion-log.log\", mode = \"w\")\n",
    "handler = logging.StreamHandler(logstream)\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s | %(name)s | %(levelname)s | %(message)s\"))\n",
    "log.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3255a22-fcad-4598-8a59-ff525e0a9bd3",
   "metadata": {},
   "source": [
    "## Compute Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47b6a87-7207-44a1-b71c-c42c9411ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These dates represent dynamic parameters to be fed into the appropriate API url to retreive relevant coin information from the current date to one \n",
    "#year ago in the past.\n",
    "log_info(\"Notebook has begun...\")\n",
    "log_info(\"Aquiring configurable dates.\")\n",
    "current_date = datetime.today().strftime(\"%Y-%m-%d\") #Todays date\n",
    "current_year = datetime.today().strftime(\"%Y\")\n",
    "current_month = datetime.today().strftime(\"%m\")\n",
    "current_day = datetime.today().strftime(\"%d\")\n",
    "prev_year = str(int(current_year) - 1)\n",
    "prev_date = prev_year + \"-\" + current_month + \"-\" + current_day #Date one year from today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd19ce6-581f-49ea-b5c8-38ae9672750f",
   "metadata": {},
   "source": [
    "## Configure Destination Paths for Ingested Data to be Stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453330fa-294f-4208-b4df-ed56dd113ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_info(\"Creating destination paths.\")\n",
    "data_parent_folder = \"data\"\n",
    "raw_data_folder = \"raw\"\n",
    "interim_data_folder = \"interim\"\n",
    "\n",
    "raw_data_file_path = os.path.join(parent_folder, data_parent_folder, raw_data_folder)\n",
    "interim_data_file_path = os.path.join(parent_folder, data_parent_folder, interim_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c41748-4fb5-402d-9c94-23114e976bb2",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "255747e5-56c9-492d-841a-61021db455a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step in the ingestion process with be to get both the name and unique slug of the coin. The unique slug is what will be passed to the API \n",
    "#to query the relevant data for the specific coin.\n",
    "\n",
    "coin_name = [] #Store the coin names here.\n",
    "coin_slug = [] #Store to unique coin slugs here\n",
    "coin_description = [] #Store coin descriptions as a single line here.\n",
    "\n",
    "log_info(\"Retreiving coin names.\")\n",
    "\n",
    "try :\n",
    "    coin_name_slug = json.loads(requests.get(\"https://coincodex.com/apps/coincodex/cache/all_coins.json\").text) #Api endpoint to get coin names and slug.\n",
    "    for i in range(0, 200) : #Get names and slug of 200 different coins. Allows for room for error with API querying.\n",
    "        slug = coin_name_slug[i]['ccu_slug']\n",
    "        name = coin_name_slug[i][\"name\"]\n",
    "        coin_slug.append(slug)\n",
    "        coin_name.append(name.lower().replace(\" \", \"-\").replace(\"/\", \"-\"))#Cleans name so it can be used as file name.\n",
    "        time.sleep(2) #Invoke a sleep for 2 seconds to abide by ethical scraping to not overload the server with requests.\n",
    "    log_info(\"Retreived all coin names and slugs.\")\n",
    "    \n",
    "except Exception as e :\n",
    "    log_error(\"ERROR: \" + str(e))\n",
    "    raise #Terminates program if there is an error.\n",
    "\n",
    "#From here, the next step in the data ingestion pipeline will be to use the names and slugs obtained to ingest data of 150 coins. However, note that \n",
    "#sometimes the API returns the wrong slug for a coin. To deal with this, the names and slugs of 200 coins are obtained, to allow for a margin for error\n",
    "#such that if the written code detects an error, it will simply move on to the next coin.\n",
    "\n",
    "\n",
    "time.sleep(5) #Once again, for ethical reasons, sleep for 10 seconds this time as to not overwhelm the server.\n",
    "counter = 0 #Want to ingest data for 150 coins only\n",
    "\n",
    "#The next step in the ingestion process will be the actual ingestion of the data itself, and saving it.\n",
    "\n",
    "new_name = [] #Represent names of coins whose data is ingested.\n",
    "\n",
    "for slug, name in zip(coin_slug, coin_name) :\n",
    "    #Call to API endpoint to receive data for a coin with the specified slug.\n",
    "    #Pass the unique slug of the coin, and the date one year ago and todays date to the API.\n",
    "    \n",
    "    try :\n",
    "        response = requests.get(\"https://coincodex.com/api/coincodexcoins/get_historical_data_by_slug/\" + slug + \"/\" + prev_date + \"/\" + current_date + \"/1?t=5459791\")\n",
    "        if response.status_code != 200 :\n",
    "            log_warning(\"Invalid slug obtained from API with coin name: \" + name + \". Finding new coin.\") #Sometimes API returns incorrect slug.\n",
    "        else :\n",
    "            coin_data = json.loads(requests.get(\"https://coincodex.com/api/coincodexcoins/get_historical_data_by_slug/\" + slug + \"/\" + prev_date + \"/\" + current_date + \"/1?t=5459791\").text)\n",
    "            coin_dataframe = pd.DataFrame(coin_data['data']) #Select all data for specified coin.\n",
    "            new_coin_dataframe = coin_dataframe.iloc[:, 2:9] #Gives all of the data in a standard format (USD).\n",
    "                \n",
    "            #Save coin data for a specific coin in a dataframe as raw data.\n",
    "        \n",
    "            with open(os.path.join(raw_data_file_path, name + \".csv\"), \"w\") as file:\n",
    "                new_coin_dataframe.to_csv(file, index = False)\n",
    "            log_info(\"SUCCESS: Data aquired for: \" + name)\n",
    "\n",
    "            coin_text_data = coin_data['coin']['description'] #Get the description for the specified coin.\n",
    "            if len(coin_text_data.strip()) == 0 :\n",
    "                coin_description.append(\"Coin: \" + name + \" has no description.\") #Default if API does not return coin description.\n",
    "                log_info(\"SUCCESS: Description aquired for: \" + name)\n",
    "                new_name.append(name)\n",
    "                counter +=1\n",
    "                time.sleep(3) #Sleep for 3 seconds for ethical reasons, as to not overwhelm the server with requests.\n",
    "            else :\n",
    "                clean_coin_text = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});') #Use a regex expression to remove html tags from the description.\n",
    "                clean_text = re.sub(clean_coin_text, '', coin_text_data) #Apply the regex expression.\n",
    "                clean_text = \"\".join(clean_text.splitlines()) #Put description on a single line in order to add it to a dataframe.\n",
    "                coin_description.append(clean_text)\n",
    "                new_name.append(name)\n",
    "                log_info(\"SUCCESS: Description aquired for: \" + name)\n",
    "                counter +=1\n",
    "                time.sleep(3) #Sleep for 3 seconds for ethical reasons, as to not overwhelm the server with requests.\n",
    "            \n",
    "            if counter == 150 : \n",
    "                #Counter to break out of program once the data for 150 coins has been ingested.\n",
    "                log_info(\"Data for 150 coins has been obtained.\")\n",
    "                break\n",
    "    \n",
    "    except Exception as e:\n",
    "        log_error(\"ERROR: \" + str(e) + slug)\n",
    "        raise\n",
    "\n",
    "coin_description_dictionary = {\"name\": new_name, \"description\": coin_description} #Create dictionary where key is coin name and value is description.\n",
    "\n",
    "coin_description_dataframe = pd.DataFrame.from_dict(coin_description_dictionary) #Convert dictionary into pandas dataframe.\n",
    "\n",
    "with open(os.path.join(interim_data_file_path, \"coin-descriptions.csv\"), \"w\") as f :\n",
    "    coin_description_dataframe.to_csv(f, index = False) #Write it to interim folder since data has been transformed and undergone intermediate cleaning.\n",
    "\n",
    "#Begin ending notebook.\n",
    "log_info(\"SUCCESS: All Coin data descriptions aquired.\")\n",
    "log.info(\"Notebook ending.\")\n",
    "logstream.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
